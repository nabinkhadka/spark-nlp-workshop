{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clinical-Entity-Assertion.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "jsl368",
      "language": "python",
      "name": "jsl368"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0Xn2cfVZUYSh"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/enterprise/healthcare/Assertion.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7GZuAzJYTzz0"
      },
      "source": [
        "<img src=\"https://nlp.johnsnowlabs.com/assets/images/logo.png\" width=\"180\" height=\"50\" style=\"float: left;\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f4BdwqEIUQ1d",
        "outputId": "26948dfd-7a11-4629-818c-8e8088c7a803",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import json\n",
        "\n",
        "with open('keys.json') as f:\n",
        "    license_keys = json.load(f)\n",
        "\n",
        "license_keys.keys()\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['version', 'secret', 'SPARK_NLP_LICENSE', 'JSL_OCR_LICENSE', 'AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY', 'JSL_OCR_SECRET'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_9yF5h8JTzz2"
      },
      "source": [
        "# Clinical Entity Recognition with Assertion - version 2.3.4\n",
        "\n",
        "## Example for Named Entity Recognition with Assertion Pipeline\n",
        "\n",
        "A common NLP problem in biomedical aplications is to identify the presence of clinical entities in a given text. This clinical entities could be diseases, symptoms, drugs, results of clinical investigations or others.\n",
        "\n",
        "But just identifying the presence of a clinical entity in an unestructured content is not enough for most of real world applications. As clinical care is full of uncertainty, in practice many of the entities refered in a medical record will not be really present in the patient but are mentioned just as working hypothesis, or identify a condition that want to be ruled out by means of a complementary test, or a condition being prevented by an intervention (for instance \"patient was vaccinated against hepatitis B\" does not imply that patient suffering from hepatitis B). In other cases a disease is mentioned associated with a relative of the patient (as in \"Father with Alzheimer disease\") as those family history is a risk factor in diseases with a genetic component.\n",
        "\n",
        "In order to extract this information from the content the Spark-NLP enterprise version includes an Assertion annotator that based in a Machine Learning pretrained model will assign, for every entity identified, a tag that informs about the nature of that entity in terms of certainty: \"present\", \"absent\", \"hypothesis\", \"conditional\", \"associated_with_other_person\", etc.\n",
        "\n",
        "In this example we will use Spark-NLP to identify some entities present in a a list of sentences adding an assertion about their certainty."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6UxqjnNHUt7i"
      },
      "source": [
        "### Step 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2cMOa3smUyKy",
        "outputId": "cd1114e1-1845-49cc-905b-11b73f1032fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "import os\n",
        "\n",
        "# Install java\n",
        "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "! java -version\n",
        "\n",
        "secret = license_keys['secret']\n",
        "os.environ['SPARK_NLP_LICENSE'] = license_keys['SPARK_NLP_LICENSE']\n",
        "os.environ['JSL_OCR_LICENSE'] = license_keys['JSL_OCR_LICENSE']\n",
        "os.environ['AWS_ACCESS_KEY_ID']= license_keys['AWS_ACCESS_KEY_ID']\n",
        "os.environ['AWS_SECRET_ACCESS_KEY'] = license_keys['AWS_SECRET_ACCESS_KEY']\n",
        "version = license_keys['version']\n",
        "jsl_version = license_keys['jsl_version']\n",
        "\n",
        "! python -m pip install --upgrade spark-nlp-jsl==$jsl_version  --extra-index-url https://pypi.johnsnowlabs.com/$secret\n",
        "\n",
        "import sparknlp\n",
        "\n",
        "print (sparknlp.version())\n",
        "\n",
        "import json\n",
        "import os\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp_jsl.annotator import *\n",
        "from sparknlp.base import *\n",
        "import sparknlp_jsl\n",
        "\n",
        "\n",
        "\n",
        "def start(secret):\n",
        "    builder = SparkSession.builder \\\n",
        "        .appName(\"Spark NLP Licensed\") \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .config(\"spark.driver.memory\", \"16G\") \\\n",
        "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "        .config(\"spark.kryoserializer.buffer.max\", \"2000M\") \\\n",
        "        .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.11:\"+version) \\\n",
        "        .config(\"spark.jars\", \"https://pypi.johnsnowlabs.com/\"+secret+\"/spark-nlp-jsl-\"+jsl_version+\".jar\")\n",
        "      \n",
        "    return builder.getOrCreate()\n",
        "\n",
        "\n",
        "spark = start(secret) # if you want to start the session with custom params as in start function above\n",
        "# sparknlp_jsl.start(secret)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "openjdk version \"1.8.0_252\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_252-8u252-b09-1~18.04-b09)\n",
            "OpenJDK 64-Bit Server VM (build 25.252-b09, mixed mode)\n",
            "Looking in indexes: https://pypi.org/simple, https://pypi.johnsnowlabs.com/8zvTuUjWPt\n",
            "Collecting spark-nlp-jsl==2.5.2\n",
            "  Downloading https://pypi.johnsnowlabs.com/8zvTuUjWPt/spark-nlp-jsl/spark_nlp_jsl-2.5.2-py3-none-any.whl\n",
            "Collecting spark-nlp==2.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/b0/c272273674b5810c0909b369c57669197907a15d84bbdf058007bb909c99/spark_nlp-2.5.2-py2.py3-none-any.whl (122kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 2.8MB/s \n",
            "\u001b[?25hCollecting pyspark==2.4.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/21/f05c186f4ddb01d15d0ddc36ef4b7e3cedbeb6412274a41f26b55a650ee5/pyspark-2.4.4.tar.gz (215.7MB)\n",
            "\u001b[K     |████████████████████████████████| 215.7MB 63kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 36.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-2.4.4-py2.py3-none-any.whl size=216130388 sha256=eb7b7a52185e5de25735afcdc123c8e00f20081e96beb4827238b2ddddd23e2b\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/09/4d/0d184230058e654eb1b04467dbc1292f00eaa186544604b471\n",
            "Successfully built pyspark\n",
            "Installing collected packages: spark-nlp, py4j, pyspark, spark-nlp-jsl\n",
            "Successfully installed py4j-0.10.7 pyspark-2.4.4 spark-nlp-2.5.2 spark-nlp-jsl-2.5.2\n",
            "2.5.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "guVWWWtYTz0M"
      },
      "source": [
        "### Step 2. Clinical NER Pipeline creation\n",
        "\n",
        "In Spark-NLP annotating NLP happens through pipelines. Pipelines are made out of various Annotator steps. In our case the architecture of the Clinical Named Entity Recognition pipeline with Assertion will be:\n",
        "\n",
        "* DocumentAssembler (text -> document)\n",
        "* SentenceDetector (document -> sentence)\n",
        "* Tokenizer (sentence -> token)\n",
        "* WordEmbeddingsModel ([sentence, token] -> embeddings)\n",
        "* NerDLModel ([sentence, token, embeddings] -> ner)\n",
        "* NerConverter([sentence, token, ner] -> ner_chunk)\n",
        "* AssertionLogRegModel ([sentence, ner_chunk, embeddings] -> assertion)\n",
        "\n",
        "So from a text we end having a list of Named Entities (Patient problems, Treatments and Tests) along with their certainty assertion tags."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5X4odV4rTz0N"
      },
      "source": [
        "#### Step 2.1 Initialize all the annotators required by the pipeline\n",
        "\n",
        "The first 3 annotators of the pipeline are \"DocumentAssembler\", \"SentenceDectector\" and \"Tokenizer\":"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qonhIRzFTz0O",
        "colab": {}
      },
      "source": [
        "# Annotator that transforms a text column from dataframe into an Annotation ready for NLP\n",
        "\n",
        "documentAssembler = DocumentAssembler()\\\n",
        "  .setInputCol(\"text\")\\\n",
        "  .setOutputCol(\"document\")\n",
        "\n",
        "# Sentence Detector annotator, processes various sentences per line\n",
        "\n",
        "sentenceDetector = SentenceDetector()\\\n",
        "  .setInputCols([\"document\"])\\\n",
        "  .setOutputCol(\"sentence\")\n",
        "\n",
        "# Tokenizer splits words in a relevant format for NLP\n",
        "\n",
        "tokenizer = Tokenizer()\\\n",
        "  .setInputCols([\"sentence\"])\\\n",
        "  .setOutputCol(\"token\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AbBRaz5CTz0S"
      },
      "source": [
        "The fourth annotator in the pipeline is \"WordEmbeddingsModel\". We will download a pretrained model available from \"clinical/models\" named \"embeddings_clinical\".\n",
        "\n",
        "When running this cell your are advised to be patient. \n",
        "\n",
        "First time you call this pretrained model it needs to be downloaded in your local.\n",
        "\n",
        "The model size is about will download the embeddings_clinical corpus it takes a while.\n",
        "\n",
        "The size is about 1.7Gb and will be saved typically in your home folder as\n",
        "\n",
        "    ~HOMEFOLDER/cached_models/embeddings_clinical_en_2.0.2_2.4_1558454742956.zip\n",
        "\n",
        "Next times you call it the model is loaded from your cached copy but even in that case it needs to be indexed each time so expect waiting up to 5 minutes (depending on your machine)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cNiujLC6Tz0T",
        "outputId": "c2d2c7c0-c982-4b97-8c84-5572a4738b24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "word_embeddings = WordEmbeddingsModel.pretrained(\"embeddings_clinical\", \"en\", \"clinical/models\")\\\n",
        "  .setInputCols([\"sentence\", \"token\"])\\\n",
        "  .setOutputCol(\"embeddings\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embeddings_clinical download started this may take some time.\n",
            "Approximate size to download 1.6 GB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6h75ZQADTz0Z"
      },
      "source": [
        "The next annotator in our pipeline is the pretrained \"ner_clinical\" NerDLModel avaliable from \"clinical/models\". It requires as input the \"sentence\", \"token\" and \"embeddings\" (clinical embeddings pretrained model) and will classify each token in four categories:\n",
        "<ol>\n",
        "    <li>PROBLEM: for patient problems</li>\n",
        "    <li>TEST: for tests, labs, etc.</li>\n",
        "    <li>TREATMENT: for treatments, medicines, etc.</li>\n",
        "    <li>OTHER: for the rest of tokens.</li>\n",
        "</ol>\n",
        "\n",
        "In order to split those identified NER that are consecutive, the B prefix (as B-PROBLEM) will be used at the first token of each NER. The I prefix (as I-PROBLEM) will be used for the rest of tokens inside the NER."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mroxK47xTz0a",
        "outputId": "7344b2b9-ac03-4552-e169-f8a1410cd0cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Named Entity Recognition for clinical concepts. Includes #Problems #Diagnostics\n",
        "\n",
        "#switch to ner_clinical instead of _noncontrib for better performance, if you are in Linux or MAC\n",
        "clinical_ner = NerDLModel.pretrained(\"ner_clinical\", \"en\", \"clinical/models\") \\\n",
        "  .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\n",
        "  .setOutputCol(\"ner\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ner_clinical download started this may take some time.\n",
            "Approximate size to download 13.8 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "my-Bz_nnTz0e"
      },
      "source": [
        "The Assertion annotator requires as an input the NER entities in a chunked format so we need the NerConverter annotator to generate that \"ner_chunk\" column in the Spark dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gJsezUAGTz0e",
        "colab": {}
      },
      "source": [
        "# Named Entity Recognition concepts parser, transforms entities into CHUNKS (required for next step: assertion status)\n",
        "\n",
        "ner_converter = NerConverter() \\\n",
        "  .setInputCols([\"sentence\", \"token\", \"ner\"]) \\\n",
        "  .setOutputCol(\"ner_chunk\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sVOzXYs2Tz0j"
      },
      "source": [
        "Finally the pretrained AssertionLogRegModel named \"assertion_ml\" is included. It will classify each named entity into its assertion type: \"present\", \"absent\", \"hypothetical\", \"conditional\", \"associated_with_other_person\", etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "is4OluxWTz0k",
        "outputId": "b05011da-baee-41a6-93f9-499519c274fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Assertion Status, verifies whether a particular subject wears a condition or not, and labels the condition by status\n",
        "\n",
        "assertion = AssertionDLModel.pretrained(\"assertion_dl\", \"en\", \"clinical/models\") \\\n",
        "  .setInputCols([\"sentence\", \"ner_chunk\", \"embeddings\"]) \\\n",
        "  .setOutputCol(\"assertion\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "assertion_dl download started this may take some time.\n",
            "Approximate size to download 1.3 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5FvBqcTaTz02"
      },
      "source": [
        "#### Step 2.2 Define the NER pipeline\n",
        "\n",
        "Now we will define the actual pipeline that puts together the annotators we have created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "D17CPDrMTz04",
        "colab": {}
      },
      "source": [
        "# Build up the pipeline\n",
        "\n",
        "pipeline = Pipeline(\n",
        "    stages = [\n",
        "    documentAssembler,\n",
        "    sentenceDetector,\n",
        "    tokenizer,\n",
        "    word_embeddings,\n",
        "    clinical_ner,\n",
        "    ner_converter,\n",
        "    assertion\n",
        "  ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CPxiJ1cwTz09"
      },
      "source": [
        "### Step 3 Create a SparkDataFrame with the content\n",
        "\n",
        "Now we will create a sample Spark dataframe with some sentences. In production environments a table with several of those sentences could be distributed in a cluster and be run in large scale systems."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CgMMH9lGTz0-",
        "outputId": "3d9b49b6-9c10-4d6a-ff3a-b4b400eb47f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# We want to know more about this simple dataframe\n",
        "\n",
        "data = spark.createDataFrame([\n",
        "  [\"Patient with severe feber and sore throat\"],\n",
        "  [\"Patient shows no stomach pain\"],\n",
        "  [\"She was maintained on an epidural and PCA for pain control.\"],\n",
        "  [\"He also became short of breath when climbing a flight of stairs.\"],\n",
        "  [\"Lung tumour located at the right lower lobe\"],\n",
        "  [\"Father with Alzheimer.\"]\n",
        "]).toDF(\"text\")\n",
        "\n",
        "data.show(truncate=False)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------------------------------------------------------+\n",
            "|text                                                            |\n",
            "+----------------------------------------------------------------+\n",
            "|Patient with severe feber and sore throat                       |\n",
            "|Patient shows no stomach pain                                   |\n",
            "|She was maintained on an epidural and PCA for pain control.     |\n",
            "|He also became short of breath when climbing a flight of stairs.|\n",
            "|Lung tumour located at the right lower lobe                     |\n",
            "|Father with Alzheimer.                                          |\n",
            "+----------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "L0cRffnHTz1J"
      },
      "source": [
        "### Step 4 Create a model fiting the NER pipeline with the clinical note.\n",
        "\n",
        "Now we can use the pipeline and the sentences to generate a model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bmvdZ0GqTz1K",
        "colab": {}
      },
      "source": [
        "# We convert the pipeline into a model, train any annotator if required (not the case)\n",
        "\n",
        "model = pipeline.fit(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9cr4M2FyTz1f"
      },
      "source": [
        "### Step 5 Transform/annotate the sentences using the model.\n",
        "\n",
        "In order to process the data with the new created model we apply a transformation.\n",
        "\n",
        "This will save in a Spakr DataFrame (output) the resuls of running the model over the clinical note. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0HUpz0s_Tz1g",
        "colab": {}
      },
      "source": [
        "output = model.transform(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "w7hIVX8QTz1n"
      },
      "source": [
        "Lets print a column with the Named Entities chunked and a column with the assertion classification assigned by the model.\n",
        "\n",
        "We see for example that in the sentence \"Patient shows no stomach pain\", the sympton \"stomach pain\" has been identified but correctly asserted as \"absent\".\n",
        "\n",
        "In the case of \"She was maintained on an epidural and PCA for pain control.\" the entity \"pain control\" has been identified and asserted as \"hypothetical\". In this case the fact that the PCA effectively controled pain is not completely certain, therefore the entity is marked as an hypothesis. However the presence of an epidural procedure and a PCA are considered as certain and asserted as \"present\".\n",
        "\n",
        "In the case of \"Father with Alzheimer\" the Assertion annotator is able to identify that this condition is associated not with the patient, but with a relative."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kz_Hu4MhTz1o",
        "outputId": "8960b5cd-5b7d-4329-b48e-a91fce22c7ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "output.select(\"ner_chunk\", \"assertion\").show(truncate=False)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|ner_chunk                                                                                                                                                                                                                                          |assertion                                                                                                                                                                                    |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|[[chunk, 13, 24, severe feber, [entity -> PROBLEM, sentence -> 0, chunk -> 0], []], [chunk, 30, 40, sore throat, [entity -> PROBLEM, sentence -> 0, chunk -> 1], []]]                                                                              |[[assertion, 13, 24, present, [sentence -> 0, chunk -> 0], []], [assertion, 30, 40, present, [sentence -> 0, chunk -> 1], []]]                                                               |\n",
            "|[[chunk, 17, 28, stomach pain, [entity -> PROBLEM, sentence -> 0, chunk -> 0], []]]                                                                                                                                                                |[[assertion, 17, 28, absent, [sentence -> 0, chunk -> 0], []]]                                                                                                                               |\n",
            "|[[chunk, 22, 32, an epidural, [entity -> TREATMENT, sentence -> 0, chunk -> 0], []], [chunk, 38, 40, PCA, [entity -> TREATMENT, sentence -> 0, chunk -> 1], []], [chunk, 46, 57, pain control, [entity -> PROBLEM, sentence -> 0, chunk -> 2], []]]|[[assertion, 22, 32, present, [sentence -> 0, chunk -> 0], []], [assertion, 38, 40, present, [sentence -> 0, chunk -> 1], []], [assertion, 46, 57, present, [sentence -> 0, chunk -> 2], []]]|\n",
            "|[[chunk, 15, 29, short of breath, [entity -> PROBLEM, sentence -> 0, chunk -> 0], []]]                                                                                                                                                             |[[assertion, 15, 29, conditional, [sentence -> 0, chunk -> 0], []]]                                                                                                                          |\n",
            "|[[chunk, 0, 10, Lung tumour, [entity -> PROBLEM, sentence -> 0, chunk -> 0], []]]                                                                                                                                                                  |[[assertion, 0, 10, present, [sentence -> 0, chunk -> 0], []]]                                                                                                                               |\n",
            "|[[chunk, 12, 20, Alzheimer, [entity -> PROBLEM, sentence -> 0, chunk -> 0], []]]                                                                                                                                                                   |[[assertion, 12, 20, associated_with_someone_else, [sentence -> 0, chunk -> 0], []]]                                                                                                         |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuLhOhfMPAv0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "2816ea3e-1788-4f43-cc9e-cc503e94f19d"
      },
      "source": [
        "output.select(\"ner_chunk.result\", \"assertion.result\").show(truncate=False)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------------------+------------------------------+\n",
            "|result                          |result                        |\n",
            "+--------------------------------+------------------------------+\n",
            "|[severe feber, sore throat]     |[present, present]            |\n",
            "|[stomach pain]                  |[absent]                      |\n",
            "|[an epidural, PCA, pain control]|[present, present, present]   |\n",
            "|[short of breath]               |[conditional]                 |\n",
            "|[Lung tumour]                   |[present]                     |\n",
            "|[Alzheimer]                     |[associated_with_someone_else]|\n",
            "+--------------------------------+------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Rp4dK__-Tz1u",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}