{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://nlp.johnsnowlabs.com/assets/images/logo.png\" width=\"180\" height=\"50\" style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation NER CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, we walk-through an evaluation for a Deep Learning NER. We will check metrics such as accuracy and training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Call necessary imports and set the resource path to read local data files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.eval import *\n",
    "\n",
    "spark = sparknlp.start(include_eval=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Gather the required files**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will require a file with labeled NER entities. This file is used to evaluate the model. So, itâ€™s used for prediction and the labels as ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = \"./eng.testb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We can also use a directory with several test files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Evaluating CRF NER Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just load a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_crf download started this may take some time.\n",
      "Approximate size to download 10.1 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "ner_crf = NerCrfModel.pretrained()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we just make use of NerDLEvaluation component to compute the accuracy. Make sure you are running MLflow UI to check the results locally on the browser. Check how to setup MLflow UI [here](https://github.com/JohnSnowLabs/spark-nlp#eval-module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nerCrfEvaluation = NerCrfEvaluation(spark, test_file)\n",
    "nerCrfEvaluation.computeAccuracyModel(ner_crf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running Mlflow UI you will see an output like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of NER CRF Eval Model](images/NERCRFAnnotatorEval.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Evaluating CRF NER Annotator**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we need to include the files for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"./eng.train\"\n",
    "embeddings_file = \"./embeddings.100d.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we just define the required annotators and its parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = WordEmbeddings() \\\n",
    "            .setInputCols([\"document\", \"token\"]) \\\n",
    "            .setOutputCol(\"embeddings\") \\\n",
    "            .setEmbeddingsSource(embeddings_file, 100, \"TEXT\")\n",
    "\n",
    "ner_crf = NerCrfApproach() \\\n",
    "      .setInputCols([\"document\", \"token\", \"pos\", \"embeddings\"]) \\\n",
    "      .setLabelColumn(\"label\") \\\n",
    "      .setOutputCol(\"ner\") \\\n",
    "      .setMaxEpochs(10) \\\n",
    "      .setRandomSeed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And invoke the required method from eval module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nerCrfEvaluation = NerCrfEvaluation(spark, test_file)\n",
    "nerCrfEvaluation.computeAccuracyAnnotator(train_file, ner_crf, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running Mlflow UI you will see an output like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of NER CRF Annotator](images/NERCRFPretrainedEval.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
